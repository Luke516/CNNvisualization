{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from adain import *\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "#==================================================\n",
    "_V19In = 0x100 #0xE0\n",
    "_V19InSq = _V19In * _V19In\n",
    "_V19InHl = _V19In >> 0x1\n",
    "_V19Out = _V19In >> 0x3\n",
    "\n",
    "# VGG19 was trained by Caffe which converted images from RGB to BGR,\n",
    "# then zero-centered each color channel with respect to the ImageNet \n",
    "# dataset, without scaling.  \n",
    "_V19Mn = np.array([103.939, 116.779, 123.68], dtype = np.float32) # BGR\n",
    "_V19Lo = -_V19Mn \n",
    "_V19Hi = 255.0 - _V19Mn \n",
    "_tfV19Mn = tf.convert_to_tensor(_V19Mn)\n",
    "_tfV19Lo = tf.convert_to_tensor(_V19Lo)\n",
    "_tfV19Hi = tf.convert_to_tensor(_V19Hi)\n",
    "_V19HW = (_V19In, _V19In)\n",
    "_V19HWhlf = (_V19InHl, _V19InHl)\n",
    "_V19HWC = (_V19In, _V19In, 0x3)\n",
    "_V19XSC = (-0x1, _V19InSq, 0x3)\n",
    "_V19NSC = (None, _V19InSq, 0x3)\n",
    "_V19BHWC = (0x1, _V19In, _V19In, 0x3)\n",
    "_V19XHWC = (-0x1, _V19In, _V19In, 0x3)\n",
    "_V19NHWC = (None, _V19In, _V19In, 0x3)\n",
    "_V19B4C1 = (_V19Out, _V19Out, 0x200)\n",
    "_V19B4C1N = (None, _V19Out, _V19Out, 0x200)\n",
    "\n",
    "_YIQMl = np.array([0.114, 0.587, 0.299], dtype = np.float32) # BGR\n",
    "_YIQBa = np.dot(_YIQMl, _V19Mn)\n",
    "_YIQAx = (0x3, 0x0)\n",
    "_ADA_TKN = \"in_ada\"\n",
    "#==================================================\n",
    "\n",
    "\n",
    "vgg = VGG19(include_top = False, weights = \"imagenet\", input_shape = _V19HWC)\n",
    "############### Build the map for signature_def ################\n",
    "mdl_c = keras.Model(\n",
    "            inputs = vgg.input,\n",
    "            outputs = [\n",
    "                    vgg.get_layer(\"block1_conv1\").output,\n",
    "                    vgg.get_layer(\"block2_conv1\").output,\n",
    "                    vgg.get_layer(\"block3_conv1\").output,\n",
    "                    vgg.get_layer(\"block4_conv1\").output,\n",
    "                ],\n",
    "            name = \"vgg19\"\n",
    "        )\n",
    "mdl_s = keras.Model(\n",
    "            inputs = vgg.input,\n",
    "            outputs = [\n",
    "                    vgg.get_layer(\"block1_conv1\").output,\n",
    "                    vgg.get_layer(\"block2_conv1\").output,\n",
    "                    vgg.get_layer(\"block3_conv1\").output,\n",
    "                    vgg.get_layer(\"block4_conv1\").output,\n",
    "                ],\n",
    "            name = \"vgg19\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(input_t, input_c, input_s):\n",
    "    # Decoder\n",
    "    _fd0 = input_t\n",
    "#     _fd0 = keras.Input(shape = _V19B4C1, name = _ADA_TKN)\n",
    "    _d1_1 = keras.layers.Conv2DTranspose(\n",
    "                0x100,\n",
    "                0x3,\n",
    "                padding = \"same\", \n",
    "                activation = \"relu\"\n",
    "    )\n",
    "    _fd1_1 = _d1_1(_fd0)\n",
    "    _d1_u = keras.layers.UpSampling2D(0x2)\n",
    "    _fd1 = _d1_u(_fd1_1)\n",
    "    _d2_1 = keras.layers.Conv2DTranspose(\n",
    "            0x100,\n",
    "            0x3,\n",
    "            padding = \"same\", \n",
    "            activation = \"relu\"\n",
    "    )\n",
    "    _fd2_1 = _d2_1(_fd1)\n",
    "    _d2_2 = keras.layers.Conv2DTranspose(\n",
    "            0x100,\n",
    "            0x3,\n",
    "            padding = \"same\", \n",
    "            activation = \"relu\"\n",
    "    )\n",
    "    _fd2_2 = _d2_2(_fd2_1)\n",
    "    _d2_3 = keras.layers.Conv2DTranspose(\n",
    "            0x100,\n",
    "            0x3,\n",
    "            padding = \"same\", \n",
    "            activation = \"relu\"\n",
    "    )\n",
    "    _fd2_3 = _d2_3(_fd2_2)\n",
    "    _d2_4 = keras.layers.Conv2DTranspose(\n",
    "            0x80,\n",
    "            0x3,\n",
    "            padding = \"same\", \n",
    "            activation = \"relu\"\n",
    "    )\n",
    "    _fd2_4 = _d2_4(_fd2_3)\n",
    "    _d2_u = keras.layers.UpSampling2D(0x2)\n",
    "    _fd2 = _d2_u(_fd2_4)\n",
    "    _d3_1 = keras.layers.Conv2DTranspose(\n",
    "            0x80,\n",
    "            0x3,\n",
    "            padding = \"same\", \n",
    "            activation = \"relu\"\n",
    "    )\n",
    "    _fd3_1 = _d3_1(_fd2)\n",
    "    _d3_2 = keras.layers.Conv2DTranspose(\n",
    "            0x40,\n",
    "            0x3,\n",
    "            padding = \"same\", \n",
    "            activation = \"relu\"\n",
    "    )\n",
    "    _fd3_2 = _d3_2(_fd3_1)\n",
    "    _d3_u = keras.layers.UpSampling2D(0x2)\n",
    "    _fd3 = _d3_u(_fd3_2)\n",
    "    _d4_1 = keras.layers.Conv2DTranspose(\n",
    "            0x40,\n",
    "            0x3,\n",
    "            padding = \"same\", \n",
    "            activation = \"relu\"\n",
    "    )\n",
    "    _fd4_1 = _d4_1(_fd3)\n",
    "    _d4_2 = keras.layers.Conv2DTranspose(\n",
    "            0x3,\n",
    "            0x3,\n",
    "            padding = \"same\", \n",
    "            activation = \"relu\"\n",
    "    )\n",
    "    _fd4_2 = _d4_2(_fd4_1)\n",
    "    decoder = keras.Model(inputs=[input_c], outputs = _fd4_2, name = \"decoder\")\n",
    "    \n",
    "    return decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_c = mdl_c.input\n",
    "input_s = mdl_s.input\n",
    "\n",
    "#Encoder\n",
    "ec = mdl_c.output[-1]\n",
    "es = mdl_s.output[-1]\n",
    "\n",
    "# AdaIN\n",
    "eps = 1.0e-06\n",
    "c0 = ec\n",
    "s0 = es\n",
    "uc, vc = tf.nn.moments(c0, axes = (0x1, 0x2), keepdims = True)\n",
    "us, vs = tf.nn.moments(s0, axes = (0x1, 0x2), keepdims = True)\n",
    "sc, ss = tf.sqrt(vc + eps), tf.sqrt(vs + eps)\n",
    "nc =  ss * (c0 - uc) / sc + us\n",
    "\n",
    "# Decoder\n",
    "_decoder = decoder(nc, input_c, input_s)\n",
    "out = _decoder.output[-1]\n",
    "# Clip\n",
    "output = tf.clip_by_value(out, _tfV19Lo, _tfV19Hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0827 17:27:49.057454 140488729929472 base.py:270] Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fc58a1c9a20> and <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7fc589912e80>).\n",
      "W0827 17:27:49.058351 140488729929472 base.py:270] Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fc589899630> and <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7fc5898b0940>).\n",
      "W0827 17:27:49.059238 140488729929472 base.py:270] Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fc5898339b0> and <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7fc5897e89e8>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint...\n"
     ]
    }
   ],
   "source": [
    "# load checkpoint\n",
    "ckp = tf.train.latest_checkpoint(\"./mdl_L/\")\n",
    "if ckp : \n",
    "    print('load checkpoint...')\n",
    "    _decoder.load_weights(ckp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0827 17:27:49.436497 140488729929472 deprecation.py:323] From <ipython-input-5-3d41878ebe2a>:2: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'./model/1/saved_model.pb'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tensor_info_c = tf.compat.v1.saved_model.build_tensor_info(input_c)\n",
    "tensor_info_s = tf.compat.v1.saved_model.build_tensor_info(input_s)\n",
    "tensor_info_output = tf.compat.v1.saved_model.build_tensor_info(output)\n",
    "add_variable = False\n",
    "\n",
    "# tensor to pass in base64\n",
    "base64_img = tf.strings.as_string(output)\n",
    "base64_img = tf.reshape(base64_img, [-1])\n",
    "base64_img = tf.strings.reduce_join(base64_img, separator=',')\n",
    "base64_img = tf.io.encode_base64(base64_img)\n",
    "shape = tf.strings.as_string(output.shape[1:])\n",
    "shape = tf.strings.reduce_join(shape, separator=',')\n",
    "msg = tf.strings.join([shape, base64_img], separator='|')\n",
    "\n",
    "# map method name to tensor info\n",
    "map_dict = {}\n",
    "map_dict[output.name] = tf.compat.v1.saved_model.build_tensor_info(msg)\n",
    "##################################################################\n",
    "\n",
    "# signature method name must be CLASSIFY_METHOD_NAME, PREDICT_METHOD_NAME or REGRESS_METHOD_NAME, \n",
    "# they have different request format. please refer to https://www.tensorflow.org/tfx/serving/signature_defs \n",
    "# and https://www.tensorflow.org/tfx/serving/api_rest/    \n",
    "signature = (\n",
    "    tf.compat.v1.saved_model.signature_def_utils.build_signature_def(\n",
    "        inputs={'input_c': tensor_info_c, 'input_s': tensor_info_s,},\n",
    "        outputs=map_dict,\n",
    "        method_name=tf.compat.v1.saved_model.signature_constants.PREDICT_METHOD_NAME \n",
    "    )\n",
    ")\n",
    "\n",
    "version = 1\n",
    "export_path = './model/{}'.format(version)\n",
    "builder = tf.compat.v1.saved_model.builder.SavedModelBuilder(export_path)\n",
    "\n",
    "\n",
    "builder.add_meta_graph_and_variables(\n",
    "    tf.compat.v1.keras.backend.get_session(), [tf.compat.v1.saved_model.tag_constants.SERVING],\n",
    "    signature_def_map={'style_transfer':signature,},\n",
    "    strip_default_attrs=True\n",
    ")\n",
    "add_variable = True\n",
    "builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (tensorflow2.0)",
   "language": "python",
   "name": "tensorflow2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
