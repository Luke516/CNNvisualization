{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from adain import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "#==================================================\n",
    "_V19In = 0x100 #0xE0\n",
    "_V19InSq = _V19In * _V19In\n",
    "_V19InHl = _V19In >> 0x1\n",
    "_V19Out = _V19In >> 0x3\n",
    "\n",
    "# VGG19 was trained by Caffe which converted images from RGB to BGR,\n",
    "# then zero-centered each color channel with respect to the ImageNet \n",
    "# dataset, without scaling.  \n",
    "_V19Mn = np.array([103.939, 116.779, 123.68], dtype = np.float32) # BGR\n",
    "_V19Lo = -_V19Mn \n",
    "_V19Hi = 255.0 - _V19Mn \n",
    "_tfV19Mn = tf.convert_to_tensor(_V19Mn)\n",
    "_tfV19Lo = tf.convert_to_tensor(_V19Lo)\n",
    "_tfV19Hi = tf.convert_to_tensor(_V19Hi)\n",
    "_V19HW = (_V19In, _V19In)\n",
    "_V19HWhlf = (_V19InHl, _V19InHl)\n",
    "_V19HWC = (_V19In, _V19In, 0x3)\n",
    "_V19XSC = (-0x1, _V19InSq, 0x3)\n",
    "_V19NSC = (None, _V19InSq, 0x3)\n",
    "_V19BHWC = (0x1, _V19In, _V19In, 0x3)\n",
    "_V19XHWC = (-0x1, _V19In, _V19In, 0x3)\n",
    "_V19NHWC = (None, _V19In, _V19In, 0x3)\n",
    "_V19B4C1 = (_V19Out, _V19Out, 0x200)\n",
    "_V19B4C1N = (None, _V19Out, _V19Out, 0x200)\n",
    "\n",
    "_YIQMl = np.array([0.114, 0.587, 0.299], dtype = np.float32) # BGR\n",
    "_YIQBa = np.dot(_YIQMl, _V19Mn)\n",
    "_YIQAx = (0x3, 0x0)\n",
    "_ADA_TKN = \"in_ada\"\n",
    "#==================================================\n",
    "\n",
    "input_c = tf.compat.v1.placeholder(dtype=tf.float32, shape=(None,)+_V19HWC, name='input_c')\n",
    "input_s = tf.compat.v1.placeholder(dtype=tf.float32, shape=(None,)+_V19HWC, name='input_s')\n",
    "\n",
    "vgg = VGG19(include_top = False, weights = \"imagenet\", input_shape = _V19HWC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_c = input_c\n",
    "encode_s = input_s\n",
    "for l in vgg.layers[1:]:\n",
    "    encode_c = l(encode_c)\n",
    "    encode_s = l(encode_s)\n",
    "    l.trainable = False\n",
    "    if \"block4_conv1\" in l.name:\n",
    "        break\n",
    "print(encode_c)\n",
    "print(encode_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(input_t, input_c, input_s):\n",
    "    # Decoder\n",
    "    _fd0 = input_t\n",
    "#     _fd0 = keras.Input(shape = _V19B4C1, name = _ADA_TKN)\n",
    "    _d1_1 = keras.layers.Conv2DTranspose(\n",
    "                0x100,\n",
    "                0x3,\n",
    "                padding = \"same\", \n",
    "                activation = \"relu\"\n",
    "    )\n",
    "    _fd1_1 = _d1_1(_fd0)\n",
    "    _d1_u = keras.layers.UpSampling2D(0x2)\n",
    "    _fd1 = _d1_u(_fd1_1)\n",
    "    _d2_1 = keras.layers.Conv2DTranspose(\n",
    "            0x100,\n",
    "            0x3,\n",
    "            padding = \"same\", \n",
    "            activation = \"relu\"\n",
    "    )\n",
    "    _fd2_1 = _d2_1(_fd1)\n",
    "    _d2_2 = keras.layers.Conv2DTranspose(\n",
    "            0x100,\n",
    "            0x3,\n",
    "            padding = \"same\", \n",
    "            activation = \"relu\"\n",
    "    )\n",
    "    _fd2_2 = _d2_2(_fd2_1)\n",
    "    _d2_3 = keras.layers.Conv2DTranspose(\n",
    "            0x100,\n",
    "            0x3,\n",
    "            padding = \"same\", \n",
    "            activation = \"relu\"\n",
    "    )\n",
    "    _fd2_3 = _d2_3(_fd2_2)\n",
    "    _d2_4 = keras.layers.Conv2DTranspose(\n",
    "            0x80,\n",
    "            0x3,\n",
    "            padding = \"same\", \n",
    "            activation = \"relu\"\n",
    "    )\n",
    "    _fd2_4 = _d2_4(_fd2_3)\n",
    "    _d2_u = keras.layers.UpSampling2D(0x2)\n",
    "    _fd2 = _d2_u(_fd2_4)\n",
    "    _d3_1 = keras.layers.Conv2DTranspose(\n",
    "            0x80,\n",
    "            0x3,\n",
    "            padding = \"same\", \n",
    "            activation = \"relu\"\n",
    "    )\n",
    "    _fd3_1 = _d3_1(_fd2)\n",
    "    _d3_2 = keras.layers.Conv2DTranspose(\n",
    "            0x40,\n",
    "            0x3,\n",
    "            padding = \"same\", \n",
    "            activation = \"relu\"\n",
    "    )\n",
    "    _fd3_2 = _d3_2(_fd3_1)\n",
    "    _d3_u = keras.layers.UpSampling2D(0x2)\n",
    "    _fd3 = _d3_u(_fd3_2)\n",
    "    _d4_1 = keras.layers.Conv2DTranspose(\n",
    "            0x40,\n",
    "            0x3,\n",
    "            padding = \"same\", \n",
    "            activation = \"relu\"\n",
    "    )\n",
    "    _fd4_1 = _d4_1(_fd3)\n",
    "    _d4_2 = keras.layers.Conv2DTranspose(\n",
    "            0x3,\n",
    "            0x3,\n",
    "            padding = \"same\", \n",
    "            activation = \"relu\"\n",
    "    )\n",
    "    _fd4_2 = _d4_2(_fd4_1)\n",
    "    return _fd4_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Encoder\n",
    "\n",
    "ec = encode_c\n",
    "es = encode_s\n",
    "\n",
    "# AdaIN\n",
    "# def post_process(x):\n",
    "#     eps = 1.0e-06\n",
    "#     c0 = x[0]\n",
    "#     s0 = x[1]\n",
    "#     uc, vc = tf.nn.moments(c0, axes = (0x1, 0x2), keep_dims = True)\n",
    "#     us, vs = tf.nn.moments(s0, axes = (0x1, 0x2), keep_dims = True)\n",
    "#     sc, ss = tf.sqrt(vc + eps), tf.sqrt(vs + eps)\n",
    "#     nc =  ss * (c0 - uc) / sc + us\n",
    "#     return nc\n",
    "    \n",
    "eps = 1.0e-06\n",
    "c0 = ec\n",
    "s0 = es\n",
    "uc, vc = tf.nn.moments(c0, axes = (0x1, 0x2), keepdims = True)\n",
    "us, vs = tf.nn.moments(s0, axes = (0x1, 0x2), keepdims = True)\n",
    "sc, ss = tf.sqrt(vc + eps), tf.sqrt(vs + eps)\n",
    "nc =  ss * (c0 - uc) / sc + us\n",
    "# layer = keras.layers.Lambda(post_process)\n",
    "# output = layer([mdl_c.output, mdl_s.output])\n",
    "\n",
    "# nc\n",
    "out = decoder(nc, input_c, input_s)\n",
    "# out = _decoder.output[-1]\n",
    "# Clip\n",
    "output = tf.clip_by_value(out, _tfV19Lo, _tfV19Hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "ckp = tf.train.latest_checkpoint(\"./mdl_L/\")\n",
    "print_tensors_in_checkpoint_file(file_name='./mdl_L/ckpt_8', tensor_name='', all_tensors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = \"\"\"\n",
    "layer_with_weights-0/bias/.ATTRIBUTES/VARIABLE_VALUE (DT_FLOAT) [256]\n",
    "layer_with_weights-0/kernel/.ATTRIBUTES/VARIABLE_VALUE (DT_FLOAT) [3,3,256,512]\n",
    "layer_with_weights-1/bias/.ATTRIBUTES/VARIABLE_VALUE (DT_FLOAT) [256]\n",
    "layer_with_weights-1/kernel/.ATTRIBUTES/VARIABLE_VALUE (DT_FLOAT) [3,3,256,256]\n",
    "layer_with_weights-2/bias/.ATTRIBUTES/VARIABLE_VALUE (DT_FLOAT) [256]\n",
    "layer_with_weights-2/kernel/.ATTRIBUTES/VARIABLE_VALUE (DT_FLOAT) [3,3,256,256]\n",
    "layer_with_weights-3/bias/.ATTRIBUTES/VARIABLE_VALUE (DT_FLOAT) [256]\n",
    "layer_with_weights-3/kernel/.ATTRIBUTES/VARIABLE_VALUE (DT_FLOAT) [3,3,256,256]\n",
    "layer_with_weights-4/bias/.ATTRIBUTES/VARIABLE_VALUE (DT_FLOAT) [128]\n",
    "layer_with_weights-4/kernel/.ATTRIBUTES/VARIABLE_VALUE (DT_FLOAT) [3,3,128,256]\n",
    "layer_with_weights-5/bias/.ATTRIBUTES/VARIABLE_VALUE (DT_FLOAT) [128]\n",
    "layer_with_weights-5/kernel/.ATTRIBUTES/VARIABLE_VALUE (DT_FLOAT) [3,3,128,128]\n",
    "layer_with_weights-6/bias/.ATTRIBUTES/VARIABLE_VALUE (DT_FLOAT) [64]\n",
    "layer_with_weights-6/kernel/.ATTRIBUTES/VARIABLE_VALUE (DT_FLOAT) [3,3,64,128]\n",
    "layer_with_weights-7/bias/.ATTRIBUTES/VARIABLE_VALUE (DT_FLOAT) [64]\n",
    "layer_with_weights-7/kernel/.ATTRIBUTES/VARIABLE_VALUE (DT_FLOAT) [3,3,64,64]\n",
    "layer_with_weights-8/bias/.ATTRIBUTES/VARIABLE_VALUE (DT_FLOAT) [3]\n",
    "layer_with_weights-8/kernel/.ATTRIBUTES/VARIABLE_VALUE (DT_FLOAT) [3,3,3,64]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_weights = []\n",
    "with tf.compat.v1.variable_scope('', reuse=tf.compat.v1.AUTO_REUSE):\n",
    "    var_list1 = [name for name in np.array(names.split()).reshape([-1, 3])[:,0] if 'kernel' in name]\n",
    "    var_list2 = [v for v in tf.compat.v1.trainable_variables() if 'conv2d_transpose' in v.name and 'kernel' in v.name]\n",
    "    for name, v in zip(var_list1, var_list2):\n",
    "        print(name, v)\n",
    "        v2 = tf.compat.v1.get_variable(name=name, shape=v.shape)\n",
    "        assign_weights.append(v.assign(v2))\n",
    "    var_list1 = [name for name in np.array(names.split()).reshape([-1, 3])[:,0] if 'bias' in name]\n",
    "    var_list2 = [v for v in tf.compat.v1.trainable_variables() if 'conv2d_transpose' in v.name and 'bias' in v.name]\n",
    "    for name, v in zip(var_list1, var_list2):\n",
    "        print(name, v)\n",
    "        v2 = tf.compat.v1.get_variable(name=name, shape=v.shape)\n",
    "        assign_weights.append(v.assign(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load checkpoint\n",
    "ckp = tf.train.latest_checkpoint(\"./mdl_L/\")\n",
    "var_list1 = [v for v in tf.compat.v1.trainable_variables() if 'layer_with_weights' in v.name]\n",
    "if ckp : \n",
    "    saver = tf.compat.v1.train.Saver(var_list=var_list1)\n",
    "    print('load checkpoint...')\n",
    "    saver.restore(tf.compat.v1.keras.backend.get_session(), ckp)\n",
    "    tf.compat.v1.keras.backend.get_session().run(assign_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_info_c = tf.compat.v1.saved_model.build_tensor_info(input_c)\n",
    "tensor_info_s = tf.compat.v1.saved_model.build_tensor_info(input_s)\n",
    "tensor_info_output = tf.compat.v1.saved_model.build_tensor_info(output)\n",
    "add_variable = False\n",
    "\n",
    "# tensor to pass in base64\n",
    "img = tf.cast(output, tf.int32)\n",
    "# base64_img = tf.strings.as_string(output)\n",
    "# base64_img = tf.reshape(base64_img, [-1])\n",
    "# base64_img = tf.strings.reduce_join(base64_img, separator=',')\n",
    "# base64_img = tf.io.encode_base64(base64_img)\n",
    "# shape = tf.strings.as_string(output.shape[1:])\n",
    "# shape = tf.strings.reduce_join(shape, separator=',')\n",
    "# msg = tf.strings.join([shape, base64_img], separator='|')\n",
    "\n",
    "# map method name to tensor info\n",
    "map_dict = {}\n",
    "map_dict[output.name] = tf.compat.v1.saved_model.build_tensor_info(img)\n",
    "##################################################################\n",
    "\n",
    "# signature method name must be CLASSIFY_METHOD_NAME, PREDICT_METHOD_NAME or REGRESS_METHOD_NAME, \n",
    "# they have different request format. please refer to https://www.tensorflow.org/tfx/serving/signature_defs \n",
    "# and https://www.tensorflow.org/tfx/serving/api_rest/    \n",
    "signature = (\n",
    "    tf.compat.v1.saved_model.signature_def_utils.build_signature_def(\n",
    "        inputs={'input_c': tensor_info_c, 'input_s': tensor_info_s,},\n",
    "        outputs=map_dict,\n",
    "        method_name=tf.compat.v1.saved_model.signature_constants.PREDICT_METHOD_NAME \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 1\n",
    "export_path = './model/{}'.format(version)\n",
    "builder = tf.compat.v1.saved_model.builder.SavedModelBuilder(export_path)\n",
    "\n",
    "\n",
    "builder.add_meta_graph_and_variables(\n",
    "    tf.compat.v1.keras.backend.get_session(), [tf.compat.v1.saved_model.tag_constants.SERVING],\n",
    "    signature_def_map={'style_transfer':signature,},\n",
    "    strip_default_attrs=True\n",
    ")\n",
    "builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir ./model/1 --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (tensorflow2.0)",
   "language": "python",
   "name": "tensorflow2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
