{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Visualization\n",
    "1. Activation maps\n",
    "2. Filter using gradient ascent\n",
    "3. Deep dream using gradient ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=\"\"\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import tensorflow as tf \n",
    "import tensorflow.keras as keras\n",
    "tf.config.gpu.set_per_process_memory_growth(True)\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "sess = tf.compat.v1.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a pretrained model like VGG16 and VGG19 trained on ImageNet to do the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg19 = keras.applications.VGG19(include_top=False, input_shape=(224, 224, 3), weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg19.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0826 11:13:19.557641 140172025980736 deprecation.py:323] From <ipython-input-5-c99114096a6a>:2: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    }
   ],
   "source": [
    "############### Build the map for signature_def ################\n",
    "inputs = tf.compat.v1.saved_model.build_tensor_info(vgg19.input)\n",
    "\n",
    "# map method name to tensor info\n",
    "map_dict = []\n",
    "for l in vgg19.layers[-1:]:\n",
    "    base64_img = tf.strings.as_string(l.output)\n",
    "    base64_img = tf.reshape(base64_img, [-1])\n",
    "    base64_img = tf.strings.reduce_join(base64_img, separator=',')\n",
    "    base64_img = tf.io.encode_base64(base64_img)\n",
    "    shape = tf.strings.as_string(l.output.shape[1:])\n",
    "    shape = tf.strings.reduce_join(shape, separator=',')\n",
    "    msg = tf.strings.join([shape, base64_img], separator='|')\n",
    "    map_dict.append((l.name, tf.compat.v1.saved_model.build_tensor_info(msg)))\n",
    "map_dict = dict(map_dict)\n",
    "\n",
    "# signature method name must be CLASSIFY_METHOD_NAME, PREDICT_METHOD_NAME or REGRESS_METHOD_NAME, \n",
    "# they have different request format. please refer to https://www.tensorflow.org/tfx/serving/signature_defs \n",
    "# and https://www.tensorflow.org/tfx/serving/api_rest/    \n",
    "signature = (\n",
    "    tf.compat.v1.saved_model.signature_def_utils.build_signature_def(\n",
    "        inputs={'input': inputs},\n",
    "        outputs=map_dict,\n",
    "        method_name=tf.compat.v1.saved_model.signature_constants.PREDICT_METHOD_NAME \n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deconvnet_model(model, layer_name):\n",
    "    deconv_layers = []\n",
    "    for i in range(len(model.layers)):\n",
    "        if isinstance(model.layers[i], keras.layers.Conv2D):\n",
    "            weights = model.layers[i].get_weights()[0]\n",
    "            weights_h, weights_w, inp_filters, out_filters = weights.shape\n",
    "            deconv_layer = keras.layers.Conv2DTranspose(inp_filters, (weights_h, weights_w),\n",
    "                                                  input_shape=(model.layers[i].output_shape[1:]),\n",
    "                                                  strides=model.layers[i].get_config()['strides'],\n",
    "                                                  padding=model.layers[i].get_config()['padding'],\n",
    "                                                  activation=model.layers[i].get_config()['activation'],\n",
    "                                                  kernel_initializer=tf.constant_initializer(weights), \n",
    "                                                  bias_initializer=tf.zeros_initializer())\n",
    "            deconv_layers.append(deconv_layer)\n",
    "        elif isinstance(model.layers[i], keras.layers.MaxPooling2D):\n",
    "            deconv_layer = keras.layers.UpSampling2D(model.layers[i].get_config()['pool_size'], interpolation=\"bilinear\")\n",
    "            deconv_layers.append(deconv_layer)\n",
    "        \n",
    "        elif isinstance(model.layers[i], keras.layers.AveragePooling2D):\n",
    "            deconv_layer = keras.layers.UpSampling2D(model.layers[i].get_config()['pool_size'], interpolation=\"bilinear\")\n",
    "            deconv_layers.append(deconv_layer)\n",
    "            \n",
    "        if model.layers[i].name == layer_name:\n",
    "            break\n",
    "    deconv_model = tf.keras.Sequential(deconv_layers[::-1])\n",
    "    return deconv_model\n",
    "\n",
    "# visualize the filter activation using deconvolution model.\n",
    "def post_process(img):\n",
    "    img -= tf.reduce_min(img, axis=[1,2,3], keepdims=True)\n",
    "    img *= 1.0 / (tf.reduce_max(img, axis=[1,2,3], keepdims=True) + 1e-8)\n",
    "    img = tf.cast(img * 255, tf.int32)\n",
    "    return img\n",
    "\n",
    "def visualization_using_deconvolution(orig_model, deconv_model, vis_image, feature_to_visualize):\n",
    "    vis_image = tf.image.resize(vis_image, (224, 224))\n",
    "\n",
    "    vis_image = orig_model(vis_image)\n",
    "    feature_map = vis_image[:, :, :, feature_to_visualize:feature_to_visualize+1] # shape [batch_size, width, height, 1]\n",
    "    paddings = [[0,0],\n",
    "                [0,0],\n",
    "                [0,0],\n",
    "                [feature_to_visualize,vis_image.shape[-1]-feature_to_visualize-1]]\n",
    "    output = tf.pad(feature_map, paddings, 'CONSTANT')\n",
    "    deconv_output = deconv_model(output)\n",
    "    return deconv_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vis_layer = 'block4_conv1'\n",
    "target_output = vgg19.get_layer(vis_layer).output \n",
    "vgg19_submodel = tf.keras.Model(vgg19.input, target_output)\n",
    "vgg19_deconv = deconvnet_model(vgg19, vis_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vis_outputs = []\n",
    "img = tf.compat.v1.placeholder(dtype=tf.float32, shape=[1, 224, 224, 3])\n",
    "for idx in range(16):\n",
    "    vis_output = visualization_using_deconvolution(vgg19_submodel, vgg19_deconv, img, idx)\n",
    "    vis_outputs.append(post_process(vis_output))\n",
    "layer_outputs = tf.stack(vis_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############### Build the map for signature_def ################\n",
    "inputs = tf.compat.v1.saved_model.build_tensor_info(img)\n",
    "\n",
    "# map method name to tensor info\n",
    "map_dict = []\n",
    "base64_img = tf.strings.as_string(img)\n",
    "base64_img = tf.reshape(base64_img, [-1])\n",
    "base64_img = tf.strings.reduce_join(base64_img, separator=',')\n",
    "base64_img = tf.io.encode_base64(base64_img)\n",
    "shape = tf.strings.as_string(layer_outputs.shape[1:])\n",
    "shape = tf.strings.reduce_join(shape, separator=',')\n",
    "msg = tf.strings.join([shape, base64_img], separator='|')\n",
    "map_dict.append((layer_outputs.name, tf.compat.v1.saved_model.build_tensor_info(msg)))\n",
    "map_dict = dict(map_dict)\n",
    "\n",
    "# signature method name must be CLASSIFY_METHOD_NAME, PREDICT_METHOD_NAME or REGRESS_METHOD_NAME, \n",
    "# they have different request format. please refer to https://www.tensorflow.org/tfx/serving/signature_defs \n",
    "# and https://www.tensorflow.org/tfx/serving/api_rest/    \n",
    "signature2 = (\n",
    "    tf.compat.v1.saved_model.signature_def_utils.build_signature_def(\n",
    "        inputs={'input': inputs},\n",
    "        outputs=map_dict,\n",
    "        method_name=tf.compat.v1.saved_model.signature_constants.PREDICT_METHOD_NAME \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'./model/1/saved_model.pb'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version = 1\n",
    "export_path = './model/{}'.format(version)\n",
    "builder = tf.compat.v1.saved_model.builder.SavedModelBuilder(export_path)\n",
    "\n",
    "builder.add_meta_graph_and_variables(\n",
    "    tf.compat.v1.keras.backend.get_session(), [tf.compat.v1.saved_model.tag_constants.SERVING],\n",
    "    signature_def_map={'activation':signature,\n",
    "                       'deconv':signature2},\n",
    "    strip_default_attrs=True\n",
    ")\n",
    "\n",
    "# # I don't know why, but can't use same tag here, or their key mix?\n",
    "builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['activation']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 224, 224, 3)\n",
      "        name: input_1:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['block5_pool'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: ()\n",
      "        name: StringJoin:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "\n",
      "signature_def['deconv']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (1, 224, 224, 3)\n",
      "        name: Placeholder_32:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['stack:0'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: ()\n",
      "        name: StringJoin_1:0\n",
      "  Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir ./model/1 --all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3(tensorflow2.0)",
   "language": "python",
   "name": "tensorflow2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
